{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df = pd.read_csv('../datasets/ames_df_explored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_test = pd.read_csv('../datasets/ames_test_explored.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Feature Selection and Using Linear Regression, Ridge, and Lasso to Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the data, I use the Stepwise Selection algorithm ot pick the best features. Stepwise works by starting with zero features and adding features one by one to find the optimal features to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I will split the data into training and testing sets so that I don't clean the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using train_test_split to create training and test sets. Set a random seed for reproducibility\n",
    "X = ames_df.loc[:,ames_df.columns != 'saleprice']\n",
    "y = ames_df['saleprice']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data with StandardScaler, to ensure that the features are compared on an even footing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data using StandardScaler()\n",
    "ames_ss = StandardScaler()\n",
    "\n",
    "ames_ss.fit(X_train)\n",
    "\n",
    "X_train_ss = ames_ss.transform(X_train)\n",
    "X_test_ss = ames_ss.transform(X_test)\n",
    "ames_test_ss = ames_ss.transform(ames_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linking the transformed data with the feature names by creating a separate dataframe\n",
    "transformed_X_train = pd.DataFrame(data=X_train_ss, columns= X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>ms_subclass</th>\n",
       "      <th>lot_frontage</th>\n",
       "      <th>lot_area</th>\n",
       "      <th>lot_shape</th>\n",
       "      <th>utilities</th>\n",
       "      <th>land_slope</th>\n",
       "      <th>overall_qual</th>\n",
       "      <th>overall_cond</th>\n",
       "      <th>...</th>\n",
       "      <th>garage_type_CarPort</th>\n",
       "      <th>garage_type_Detchd</th>\n",
       "      <th>sale_type_CWD</th>\n",
       "      <th>sale_type_Con</th>\n",
       "      <th>sale_type_ConLD</th>\n",
       "      <th>sale_type_ConLI</th>\n",
       "      <th>sale_type_ConLw</th>\n",
       "      <th>sale_type_New</th>\n",
       "      <th>sale_type_Oth</th>\n",
       "      <th>sale_type_WD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.036852</td>\n",
       "      <td>-1.036852</td>\n",
       "      <td>0.322826</td>\n",
       "      <td>1.847527</td>\n",
       "      <td>0.531987</td>\n",
       "      <td>0.720483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.223345</td>\n",
       "      <td>0.62824</td>\n",
       "      <td>2.165189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057148</td>\n",
       "      <td>-0.598406</td>\n",
       "      <td>-0.067662</td>\n",
       "      <td>-0.036108</td>\n",
       "      <td>-0.092389</td>\n",
       "      <td>-0.025524</td>\n",
       "      <td>-0.057148</td>\n",
       "      <td>-0.288471</td>\n",
       "      <td>-0.051098</td>\n",
       "      <td>0.389133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  ms_subclass  lot_frontage  lot_area  lot_shape  \\\n",
       "0   -1.036852     -1.036852     0.322826      1.847527  0.531987   0.720483   \n",
       "\n",
       "   utilities  land_slope  overall_qual  overall_cond  ...  \\\n",
       "0        0.0    0.223345       0.62824      2.165189  ...   \n",
       "\n",
       "   garage_type_CarPort  garage_type_Detchd  sale_type_CWD  sale_type_Con  \\\n",
       "0            -0.057148           -0.598406      -0.067662      -0.036108   \n",
       "\n",
       "   sale_type_ConLD  sale_type_ConLI  sale_type_ConLw  sale_type_New  \\\n",
       "0        -0.092389        -0.025524        -0.057148      -0.288471   \n",
       "\n",
       "   sale_type_Oth  sale_type_WD   \n",
       "0      -0.051098       0.389133  \n",
       "\n",
       "[1 rows x 176 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making sure the dataframe is created properly\n",
    "transformed_X_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Stepwise Selection to pick the best features. I will use the implementation from the library Machine Learning Extensions: [mlxtend](https://github.com/rasbt/mlxtend/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate linear regression for use in Sequential Feature Selection\n",
    "ames_lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do Sequential Feature Selector with the Linear Regression model to pick the best k features,\n",
    "#with k between 30-40, after selection was tested with 1-100. \n",
    "ames_sfs = SequentialFeatureSelector(estimator=ames_lr, k_features=(10,30), forward=True, \n",
    "                                     scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialFeatureSelector(estimator=LinearRegression(), k_features=(10, 30),\n",
       "                          scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames_sfs.fit(transformed_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is: -24073.60555382976, \n",
      "\n",
      "Achieved using these features: ('ms_subclass', 'lot_area', 'overall_qual', 'overall_cond', 'year_built', 'mas_vnr_area', 'exter_qual', 'bsmt_qual', 'bsmt_exposure', 'bsmtfin_sf_1', 'total_bsmt_sf', 'gr_liv_area', 'bedroom_abvgr', 'kitchen_qual', 'functional', 'screen_porch', 'land_contour_HLS', 'neighborhood_BrkSide', 'neighborhood_Crawfor', 'neighborhood_NoRidge', 'neighborhood_NridgHt', 'neighborhood_StoneBr', 'condition_1_Norm', 'condition_1_PosN', 'condition_2_PosA', 'roof_style_Hip', 'exterior_1st_BrkFace', 'mas_vnr_type_BrkFace', 'garage_type_Attchd', 'sale_type_New')\n"
     ]
    }
   ],
   "source": [
    "#Printing the best score found by the Sequential Feature Selector and the feature names\n",
    "print(f\"The best score is: {ames_sfs.k_score_}, \\n\\nAchieved using these features: {ames_sfs.k_feature_names_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 30 features\n"
     ]
    }
   ],
   "source": [
    "#Saving the feature names into a list\n",
    "features_to_use = list(ames_sfs.k_feature_names_)\n",
    "print(f\"There are a total of {len(ames_sfs.k_feature_names_)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the X_train, X_test, and test set data and save them into variables\n",
    "X_train_sfs = ames_sfs.transform(X_train_ss)\n",
    "X_test_sfs = ames_sfs.transform(X_test_ss)\n",
    "ames_test_sfs = ames_sfs.transform(ames_test_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate, Fit, and predict for Baseline Model, Linear Regression, RidgeCV, LassoCV, and ElasticNetCV \n",
    "#given a range of values for alpha and l1_ratio (for ElasticNet).\n",
    "\n",
    "#Baseline Model using the mean\n",
    "ames_baseline = y_test.copy()\n",
    "ames_baseline[ames_baseline > 0] = ames_baseline.mean()\n",
    "\n",
    "#LinearRegression\n",
    "ames_lr = LinearRegression()\n",
    "ames_lr.fit(X_train_sfs, y_train)\n",
    "y_pred_lr = ames_lr.predict(X_test_sfs)\n",
    "\n",
    "#Ridgecv\n",
    "ames_ridgecv = RidgeCV(alphas= np.logspace(0, 5, 200), cv=5)\n",
    "ames_ridgecv_model = ames_ridgecv.fit(X_train_sfs, y_train)\n",
    "y_pred_ridgecv = ames_ridgecv_model.predict(X_test_sfs)\n",
    "\n",
    "#Lassocv\n",
    "ames_lassocv = LassoCV(alphas= np.arange(.0001, 10, 0.01), cv=5)\n",
    "ames_lassocv_model = ames_lassocv.fit(X_train_sfs, y_train)\n",
    "y_pred_lassocv = ames_lassocv_model.predict(X_test_sfs)\n",
    "\n",
    "#ElasticNetcv (Note, for reproduction purposes, \n",
    "#alpha was initially set at np.arange(.0001, 10, 0.01)\n",
    "#l1_ratio was initially set at np.arange(0.001, 0.5, 0.005))\n",
    "ames_enetcv = ElasticNetCV(alphas= np.arange(.001, 3, 0.01), l1_ratio=np.arange(0.1, 0.5, 0.005), cv=5)\n",
    "ames_enetcv_model = ames_enetcv.fit(X_train_sfs, y_train)\n",
    "y_pred_enetcv = ames_enetcv_model.predict(X_test_sfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Regression score: 79511.72593887136\n",
      "Linear Regression score: 24324.890922875984\n",
      "Ridge Regression score: 24408.82434724307, with an alpha of 20.255019392306675\n",
      "Lasso Regression score: 24323.02135801103, with an alpha of 6.1101\n",
      "ElasticNet Regression score: 24394.006038969565, with an alpha of 0.020999999999999998 and l1_ratio of 0.4800000000000003\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline Regression score: {mean_squared_error(y_test, ames_baseline, squared=False)}\")\n",
    "print(f\"Linear Regression score: {mean_squared_error(y_test, y_pred_lr, squared=False)}\")\n",
    "print(f\"Ridge Regression score: {mean_squared_error(y_test, y_pred_ridgecv, squared=False)},\\\n",
    " with an alpha of {ames_ridgecv_model.alpha_}\")\n",
    "print(f\"Lasso Regression score: {mean_squared_error(y_test, y_pred_lassocv, squared=False)},\\\n",
    " with an alpha of {ames_lassocv_model.alpha_}\")\n",
    "print(f\"ElasticNet Regression score: {mean_squared_error(y_test, y_pred_enetcv, squared=False)},\\\n",
    " with an alpha of {ames_enetcv_model.alpha_} and l1_ratio of {ames_enetcv_model.l1_ratio_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Predicting Prices and saving the production model to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new test set to grab Id column\n",
    "test_set_id = pd.read_csv('../datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the mean_squared_error score for each model. The lower the score the better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_scores = pd.DataFrame(ames_lassocv_model.predict(ames_test_sfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_scores.rename(columns = {0:'SalePrice'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_scores['Id'] = test_set_id['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_scores = predicted_test_scores.reindex(columns= ['Id', 'SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Interpreting models and explaining production model choice\n",
    "\n",
    "All the 4 models listed above are regression models. The basic interpretation is that with every increase in an x, the response variable y will increase by coefficient beta times. \n",
    "\n",
    "#### Ridge and Lasso regression\n",
    "\n",
    "Ridge and Lasso regressions are regressions that control for high variance in models, meaning that they control the amount of complexity the models have. They both do this by adding a penalty term to the basic linear regression equation. The difference between the two is how strongly this penalty term is weighted. Ridge regression takes the square of the penalty term, while Lasso simply takes the absolute value of the penalty term.\n",
    "\n",
    "For practical purposes, you can think of Ridge and Lasso regressions as 'reducing the strength' of variable coefficients. For example, if one variable x has coefficient 10,000, both regressions might reduce the strength to about 1,000. The difference between Ridge and Lasso is **how much** and **how quickly** the coefficient size is reduced.\n",
    "\n",
    "Ridge regression reduces linear regression coefficients gradually, with a steep decrease at the start and a tapered decrease later on. However, the coefficients will never disappear. They may be small, but **all the features used in Ridge regression will remain in the model**.\n",
    "\n",
    "For Lasso regression, the decrease in coefficient size is very quick, almost linearly straight down. Coefficients can be set to 0, meaning that **some features will not be used in the model any longer**.\n",
    "\n",
    "#### ElasticNet regression\n",
    "\n",
    "ElasticNet is simply a mix of both Lasso and Ridge regressions, weighted with a lambda variable. If lambda=1, ElasticNet is using purely Lasso regression. If lambda=0, it's using purely Ridge regression. if lambda is anywhere between 0 and 1, it's using a mix of both regressions.\n",
    "\n",
    "#### Choosing the production model\n",
    "\n",
    "For my models above, the scores are around the same. This is not surprising as the best features were already selected by Sequential Feature Selector. ElasticNet did worse than either Lasso or Ridge on a default setting of 0.5 and only scored around the same as Ridge and Lasso regression. The optimal mix of Lasso and Ridge regressions, at 0.456, was still not as good as purely Lasso.\n",
    "\n",
    "I will pick the Lasso regression model because it gives the same score as Linear Regression. While I could have picked Linear Regression, the fact is that linear regression gives the 'baseline' score. Lasso Regression is still slightly better.\n",
    "\n",
    "Let's see the features and the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab the coefficients for Lasso Regression, which gives a slightly better score than Linear and Ridge regression\n",
    "best_lasso_coef = pd.DataFrame(data = [features_to_use, ames_lassocv_model.coef_])\n",
    "best_lasso_coef.columns = best_lasso_coef.iloc[0]\n",
    "best_lasso_coef.drop(best_lasso_coef.index[[0]], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ms_subclass</th>\n",
       "      <th>lot_area</th>\n",
       "      <th>overall_qual</th>\n",
       "      <th>overall_cond</th>\n",
       "      <th>year_built</th>\n",
       "      <th>mas_vnr_area</th>\n",
       "      <th>exter_qual</th>\n",
       "      <th>bsmt_qual</th>\n",
       "      <th>bsmt_exposure</th>\n",
       "      <th>bsmtfin_sf_1</th>\n",
       "      <th>...</th>\n",
       "      <th>neighborhood_NridgHt</th>\n",
       "      <th>neighborhood_StoneBr</th>\n",
       "      <th>condition_1_Norm</th>\n",
       "      <th>condition_1_PosN</th>\n",
       "      <th>condition_2_PosA</th>\n",
       "      <th>roof_style_Hip</th>\n",
       "      <th>exterior_1st_BrkFace</th>\n",
       "      <th>mas_vnr_type_BrkFace</th>\n",
       "      <th>garage_type_Attchd</th>\n",
       "      <th>sale_type_New</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6023.33</td>\n",
       "      <td>6453.41</td>\n",
       "      <td>12542.5</td>\n",
       "      <td>5530.93</td>\n",
       "      <td>10113.4</td>\n",
       "      <td>5961.22</td>\n",
       "      <td>6069.91</td>\n",
       "      <td>3934.69</td>\n",
       "      <td>5532.54</td>\n",
       "      <td>9784.84</td>\n",
       "      <td>...</td>\n",
       "      <td>7383.45</td>\n",
       "      <td>5509.17</td>\n",
       "      <td>2713.46</td>\n",
       "      <td>1971.75</td>\n",
       "      <td>2208.07</td>\n",
       "      <td>2235.78</td>\n",
       "      <td>2553.95</td>\n",
       "      <td>-4317.12</td>\n",
       "      <td>-1991.53</td>\n",
       "      <td>6196.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0 ms_subclass lot_area overall_qual overall_cond year_built mas_vnr_area  \\\n",
       "1    -6023.33  6453.41      12542.5      5530.93    10113.4      5961.22   \n",
       "\n",
       "0 exter_qual bsmt_qual bsmt_exposure bsmtfin_sf_1  ... neighborhood_NridgHt  \\\n",
       "1    6069.91   3934.69       5532.54      9784.84  ...              7383.45   \n",
       "\n",
       "0 neighborhood_StoneBr condition_1_Norm condition_1_PosN condition_2_PosA  \\\n",
       "1              5509.17          2713.46          1971.75          2208.07   \n",
       "\n",
       "0 roof_style_Hip exterior_1st_BrkFace mas_vnr_type_BrkFace garage_type_Attchd  \\\n",
       "1        2235.78              2553.95             -4317.12           -1991.53   \n",
       "\n",
       "0 sale_type_New  \n",
       "1       6196.48  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lasso_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Interpreting the Lasso Regression Model and Recommendations\n",
    "\n",
    "### Most Important positive factors\n",
    "\n",
    "**gr_liv_area, overall_qual, and year_built** seem to be the three strongest factors that affect housing prices. Simply increasing your above ground living area square feet by 1 increases the housing price by a wooping 30,172! Comparatively, overall_qual and year_built are about a third as strong, with an increase in overall quality by 1 signalling an increase by 12,532. Finally, every new year that arrives means that houses selling in that year are about 10,130 more expensive than the previous year!\n",
    "\n",
    "### Negative factors\n",
    "\n",
    "**ms_subclass** is the first feature we find,and it's a numerical nominal class. It starts with 020 and ends with 190 with increments of 5 (e.g. 040 -> 045) or 10 (e.g. 020 -> 030). This tells us that the higher the number, the less valuable the house is. Meaning that 2 FAMILY CONVERSION - ALL STYLES AND AGES is less valuable than 1-STORY 1946 & NEWER ALL STYLES. An increase by 10 in this variable decreases value by 60,280 (multiplied by 10 since each increment is a multiple of 10)\n",
    "\n",
    "**bedroom_abvgr**. 'abvgr' refers to a value in the discrete 'bedroom' category, refering to the number of bedrooms above ground. Apparently the more bedrooms you have above ground, the cheaper your house, by 3787. Perhaps because it is exposed to the noise of the streets, or because it's another signal for the fact that the house doesn't have bedrooms below ground.\n",
    "\n",
    "**mas_vnr_type_BrkFace**. 'BrkFace' refers to the Brick Face masonry veneer type. If you have this type of veneer, your house value drops by 4330. Perhaps this is because Brick Face is a cheaper type of veneer.\n",
    "\n",
    "**garage_type_Attchd**. Having a garage attached (Attchd) to your home means your house is cheaper by 2000. This sounds like it's a signal for how the house was built. In other words, only houses built a certain way will have a garage attached to them.\n",
    "\n",
    "### Other factors\n",
    "\n",
    "**neighbourhood_...** the elipses refers to the different types of neighborhoods the house can be located in. While none of the neighborhoods reduce prices, there are neighborhoods that can increase prices by as much as 5x with a baseline of around 1700. For example, compare BrkSide and NridgeHt, with BrkSide increasing prices by 1713 compared to NridgeHt's 7387.\n",
    "\n",
    "**lot_area, overall_qual**, are all easily understood. As the size, quality, and condition of your house increases, prices increase too. The later your house was built, the more expensive it is. The quality of your house and the year it was built has the strongest effect amonth the 4, increasing house prices by more than 10,000. lot_area and overall_cond contribute about half as much as the former 2 variables!\n",
    "\n",
    "**mas_vnr_area, exter_qual, bsmt_qual, bsmt_exposure** continue the trend of easily interpretable variables. The more veneer, the higher the quality of the exterior, the higher the quality of the basement, and the more exposure there is to walkout or garden level walls, the higher the price of the house.\n",
    "\n",
    "**bsmtfin_sf_1, total_bsmt_sf**. The more you have something called type 1 finished area in your basement, the higher your price, while total basement area makes the house more expensive.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The rest of the factors, like 'other factors' above, are easy to interpret. In the end, the bigger the house, with a better rating of quality and condition, fetches you the highest prices. Even if you have the negative factors attached to the home, they only reduce prices by a factor of 1/5 or so (i.e. prices reduce by -6000 for every 30,000 increase in price, given ms_subclass and gr_liv_area). \n",
    "\n",
    "#### For Companies\n",
    "\n",
    "For a given house, make sure that the quality and condition matches the one stated on paper. While it may be possible, though not advisable, to sell a BrkfacFace veneer at a more expensive price than expected, it's impossible to hide a house's quality and condition. If your company builds and sells property, you may want to build 1 story houses instead of 2 family conversions as it gives a better return of investment. However, watch out for market saturation: too many of the same type of house and the houses won't command as much money.\n",
    "\n",
    "#### For Consumers\n",
    "\n",
    "Location, location, location matters. Assuming you are someone most concerned with price, try to go for a cheaper neighborhood. But if you wish to stay at, say, NridgeHt, it's not the end of the world. Buy a more run down home and upgrade it yourself! Look for bedrooms above ground, and try to get a house that has a garage attached to it. Get an older house, not one built recently; the older the house the better. Make sure you don't buy a house with a basement. Get a house with a Brick Face veneer and renovate the house.\n",
    "\n",
    "### Further Steps\n",
    "\n",
    "With a linear regression model built for this town, perhaps building another linear regression model on a similar dataset for another town would be useful. With the two linear models, we can compare the coefficients and find out whether the same features play the biggest part in housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "(2020, July 17). _City Assessor_. City of Ames. [https://www.cityofames.org/government/departments-divisions-a-h/city-assessor](https://www.cityofames.org/government/departments-divisions-a-h/city-assessor)\n",
    "\n",
    "Boachie, P. (2016, July 21). _5 Strategies of 'Psychological Pricing'_. The Entrepreneur. [https://www.entrepreneur.com/article/279464](https://www.entrepreneur.com/article/279464)\n",
    "\n",
    "De Cock, D. _Ames Iowa: Alternative to the Boston Housing Data Set_. Truman State University. [http://jse.amstat.org/v19n3/decock/DataDocumentation.txt](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)\n",
    "\n",
    "Kuhn, M. & Johnson, K. (2019, June 21). _Feature Engineering and Selection: A Practical Approach for Predictive Models_. Taylor & Francis Group. [https://bookdown.org/max/FES/](https://bookdown.org/max/FES/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
