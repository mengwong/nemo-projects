{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import csv file into pd and remind ourselved how it looks like with .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv('../data/combined_df_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new to solo travel  post here for introduction...</td>\n",
       "      <td>new   are you planning your first big trip to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi  can someone tell me how the restrictions w...</td>\n",
       "      <td>syou can delete this  if this is isn t suppos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is it expensive traveling with a flexible itin...</td>\n",
       "      <td>i m planning my first solo trip  hopefully  an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistakes whilst travelling solo</td>\n",
       "      <td>i have personally made many mistakes whilst tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weak passport holders  how do you travel spont...</td>\n",
       "      <td>so i plan to travel once it is safe to travel ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  new to solo travel  post here for introduction...   \n",
       "1  hi  can someone tell me how the restrictions w...   \n",
       "2  is it expensive traveling with a flexible itin...   \n",
       "3                    mistakes whilst travelling solo   \n",
       "4  weak passport holders  how do you travel spont...   \n",
       "\n",
       "                                            selftext  subreddit  \n",
       "0   new   are you planning your first big trip to...          0  \n",
       "1   syou can delete this  if this is isn t suppos...          0  \n",
       "2  i m planning my first solo trip  hopefully  an...          0  \n",
       "3  i have personally made many mistakes whilst tr...          0  \n",
       "4  so i plan to travel once it is safe to travel ...          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's just use 'selftext' for X for now\n",
    "X = combined_df['selftext']\n",
    "y = combined_df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     new   are you planning your first big trip to...\n",
       "1     syou can delete this  if this is isn t suppos...\n",
       "2    i m planning my first solo trip  hopefully  an...\n",
       "3    i have personally made many mistakes whilst tr...\n",
       "4    so i plan to travel once it is safe to travel ...\n",
       "Name: selftext, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the first 5 rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A reminder that solotravel = 0, JapanTravel = 1\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Building and scoring baseline model alongside Naive Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a baseline to compare our models against. A baseline model in this case is simply predicting the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.523274\n",
       "0    0.476726\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the normalised value counts to get the percentage\n",
    "combined_df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are almost balanced, with JapanTravel being the majority class. Simply predicting JapanTravel all the time will give you a baseline accuracy of 52%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: First Naive Bayes Model with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the classic train_test_split, stratifying on y because there's slightly imbalanced classes with about 50 more\n",
    "#JapanTravel than solotravel\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our CountVectorizer without stop words to see how well it does\n",
    "cvec = CountVectorizer(max_features = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>x200b</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  03  10  11  12  13  14  15  16  17  ...  work  world  worth  would  \\\n",
       "0   0   0   0   0   0   0   0   0   0   0  ...     1      0      0      0   \n",
       "1   0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "2   0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "\n",
       "   x200b  year  years  yet  you  your  \n",
       "0      0     0      0    0    0     0  \n",
       "1      0     1      0    0    1     0  \n",
       "2      0     0      0    0    1     0  \n",
       "\n",
       "[3 rows x 500 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit our CountVectorizer on the training data and transform training data.\n",
    "X_train_cvec = pd.DataFrame(cvec.fit_transform(X_train).toarray(), columns = cvec.get_feature_names())\n",
    "X_train_cvec.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>x200b</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  03  10  11  12  13  14  15  16  17  ...  work  world  worth  would  \\\n",
       "0   0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "1   0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "2   0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "\n",
       "   x200b  year  years  yet  you  your  \n",
       "0      0     0      0    0    1     1  \n",
       "1      0     0      0    0    0     0  \n",
       "2      0     0      0    0    1     0  \n",
       "\n",
       "[3 rows x 500 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform our testing data with the already-fit CountVectorizer.\n",
    "X_test_cvec = pd.DataFrame(cvec.transform(X_test).toarray(), columns = cvec.get_feature_names())\n",
    "X_test_cvec.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a multinomial Naive Bayes to classify the posts. We use bernoulli because this is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate bernoulli Naive Bayes\n",
    "multi_nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model on X_train_cvec and y_train\n",
    "model = multi_nb.fit(X = X_train_cvec, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on X_train_cvec\n",
    "predictions_nb_train = model.predict(X_train_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on X_test_cvec\n",
    "predictions_nb_test = model.predict(X_test_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8600999286224126"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score our model on the training set.\n",
    "model.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461538461538461"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score our model on the testing set.\n",
    "model.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be low variance and high bias in this case as the scores only differ at the 3rd decimal place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run another classification algorithm like logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3: Building a Logistic Regression model to compare with first Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set LogisticRegression parameters\n",
    "lr_params = {'penalty':['elasticnet'], \n",
    "             'random_state': [42],\n",
    "             'solver':['saga'],\n",
    "             'l1_ratio': np.arange(0.1, 1, 0.05),\n",
    "             'multi_class':['ovr'],\n",
    "            'max_iter':[150]}\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do gridsearch\n",
    "lr_gridsearch = GridSearchCV(LogisticRegression(), param_grid = lr_params, verbose=1, cv=5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   18.7s finished\n",
      "C:\\Users\\junho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "#Fitting the gridsearch model on X_train_cvec and y_train\n",
    "lr_model = lr_gridsearch.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the training and test scores respectively\n",
    "predictions_lr_train = lr_model.predict(X_train_cvec)\n",
    "predictions_lr_test = lr_model.predict(X_test_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8893647394718058"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score our model on the training set.\n",
    "lr_model.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8739316239316239"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score our model on the test set.\n",
    "lr_model.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression seems to have even lower bias, though with a slightly increased variance. However, this trade-off is for the better, as variance only increased slightly, while bias dropped by a large amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate confusion matrixes for the results\n",
    "\n",
    "The first 2 for the test results, naive bayes and logistic regression respectively.\n",
    "\n",
    "The last 2 for the training results, naive bayes and logistic regression respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function for easy scoring for both train and test scores\n",
    "def test_confusion_score(predictions):\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print('')\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "\n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp)\n",
    "    print('')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "    print(f\"Recall: {recall_score(y_test,predictions)}\")\n",
    "    print(f\"Precision: {precision_score(y_test, predictions)}\")\n",
    "    \n",
    "def train_confusion_score(predictions):\n",
    "    print(confusion_matrix(y_train, predictions))\n",
    "    print('')\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, predictions).ravel()\n",
    "\n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp)\n",
    "    print('')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\n",
    "    print(f\"Recall: {recall_score(y_train,predictions)}\")\n",
    "    print(f\"Precision: {precision_score(y_train, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for naive bayes test results:\n",
      "\n",
      "[[210  13]\n",
      " [ 59 186]]\n",
      "\n",
      "True Negatives: 210\n",
      "False Positives: 13\n",
      "False Negatives: 59\n",
      "True Positives: 186\n",
      "\n",
      "Accuracy: 0.8461538461538461\n",
      "Recall: 0.7591836734693878\n",
      "Precision: 0.9346733668341709\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix for naive bayes test results\n",
    "print(\"Confusion matrix for naive bayes test results:\")\n",
    "print('')\n",
    "test_confusion_score(predictions_nb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for logistic regression test results:\n",
      "\n",
      "[[207  16]\n",
      " [ 43 202]]\n",
      "\n",
      "True Negatives: 207\n",
      "False Positives: 16\n",
      "False Negatives: 43\n",
      "True Positives: 202\n",
      "\n",
      "Accuracy: 0.8739316239316239\n",
      "Recall: 0.8244897959183674\n",
      "Precision: 0.926605504587156\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix for logistic regression test results\n",
    "print(\"Confusion matrix for logistic regression test results:\")\n",
    "print('')\n",
    "test_confusion_score(predictions_lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for naive bayes training results:\n",
      "\n",
      "[[649  19]\n",
      " [177 556]]\n",
      "\n",
      "True Negatives: 649\n",
      "False Positives: 19\n",
      "False Negatives: 177\n",
      "True Positives: 556\n",
      "\n",
      "Accuracy: 0.8600999286224126\n",
      "Recall: 0.7585266030013642\n",
      "Precision: 0.9669565217391304\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix for naive bayes training results\n",
    "print(\"Confusion matrix for naive bayes training results:\")\n",
    "print('')\n",
    "train_confusion_score(predictions_nb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for logistic regression training results:\n",
      "\n",
      "[[632  36]\n",
      " [119 614]]\n",
      "\n",
      "True Negatives: 632\n",
      "False Positives: 36\n",
      "False Negatives: 119\n",
      "True Positives: 614\n",
      "\n",
      "Accuracy: 0.8893647394718058\n",
      "Recall: 0.8376534788540245\n",
      "Precision: 0.9446153846153846\n"
     ]
    }
   ],
   "source": [
    "#Generate a confusion matrix for logistic regression training results\n",
    "print(\"Confusion matrix for logistic regression training results:\")\n",
    "print('')\n",
    "train_confusion_score(predictions_lr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What these labels mean\n",
    "\n",
    "Remember that solotravel = 0, JapanTravel = 1,\n",
    "\n",
    "**True Negatives** refers to solotravel posts that were correctly classified as solotravel posts (correctly labelled as 0)\n",
    "\n",
    "**False Positives** refers to solotravel posts that were wrongly classified as JapanTravel posts (wrongly labelled as 1)\n",
    "\n",
    "**False Negatives** refers to JapanTravel posts that were wrongly classified as solotravel posts (wrongly labelled as 0)\n",
    "\n",
    "**True Positives** refers to JapanTravel posts that were correctly classified as JapanTravel posts (correctly labelled as 0)\n",
    "\n",
    "\n",
    "**Accuracy** refers to how many posts from both subreddits were corrrectly classified\n",
    "\n",
    "**Recall** refers to how many JapanTravel posts truly were classified as JapanTravel posts as a ratio.\n",
    "\n",
    "**Precision** refers to the number of posts that were classified as JapanTravel were actually from the JapanTravel subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both classifiers had more False Negatives than False Positives, which means that both classifiers had more trouble classifying JapanTravel posts as JapanTravel posts. We can see this in the lower recall than precision, meaning that more JapanTravel posts were classified as solotravel (lower recall), than solotravel posts being classified as JapanTravel (higher precision)\n",
    "\n",
    "As we saw in our exploration in the last notebook, solotravel posts uses fewer words and fewer unique words. It's possible that these False Negatives happen because these JapanTravel posts are shorter, with fewer words unique to JapanTravel.\n",
    "\n",
    "Logistic Regression did better than Naive Bayes. This is potentially because the Logistic Regression I ran had feature selection built into it as I used the 'elasticnet' version of Logistic Regression. This means that the most significant features (words) were given more weight than the less significant ones.\n",
    "\n",
    "I also did not attempt to improve the posts any further by removing stop words or attempt lemmatization or stemming.\n",
    "\n",
    "We can treat this Naive Bayes classifier as a baseline and attempt some stemming and remove the stop words to see if it improves the score.\n",
    "\n",
    "Let's improve the posts and see how much of an effect it has on Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Attempting to build better models with different versions of CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1: CountVectorizer removing stop words, max_df of 0.95, and min_df of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our CountVectorizer with stop words, a min_df of 2 to remove very rare words, and \n",
    "# a max_df of 0.95 to see how well it does\n",
    "cvec = CountVectorizer(max_features = 500, stop_words= 'english', max_df = 0.95, min_df = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cvec to the X_train and X_test splits\n",
    "X_train_improved = pd.DataFrame(cvec.fit_transform(X_train).toarray(), columns = cvec.get_feature_names())\n",
    "X_test_improved = pd.DataFrame(cvec.fit_transform(X_test).toarray(), columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define a function for the bernoulli and logistic model for future use\n",
    "def multi_model(instantiated_multi, X_train, X_test):\n",
    "    #Fit the model on X_train and y_train\n",
    "    model = instantiated_multi.fit(X = X_train, y = y_train)\n",
    "    \n",
    "    #Predict for both train and test scores\n",
    "    predictions_nb_train = model.predict(X_train)\n",
    "    predictions_nb_test = model.predict(X_test)\n",
    "    \n",
    "    return model, predictions_nb_train, predictions_nb_test\n",
    "\n",
    "def lr_model(X_train, X_test):\n",
    "    \n",
    "    #Same parameters as the one above. I can think of no reason why we would want to change this.\n",
    "    lr_params = {'penalty':['elasticnet'], \n",
    "             'random_state': [42],\n",
    "             'solver':['saga'],\n",
    "             'l1_ratio': np.arange(0.1, 1, 0.05),\n",
    "             'multi_class':['ovr'],\n",
    "            'max_iter':[150]}\n",
    "    \n",
    "    #Use gridsearch to iterate through the params. This is for the l1_ratio variable\n",
    "    lr_gridsearch = GridSearchCV(LogisticRegression(), param_grid = lr_params, verbose=1, cv=5, n_jobs = -1)\n",
    "    \n",
    "    #Fit the model\n",
    "    lr_model = lr_gridsearch.fit(X_train, y_train)\n",
    "    \n",
    "    #Predict for both train and test scores\n",
    "    predictions_lr_train = lr_model.predict(X_train)\n",
    "    predictions_lr_test = lr_model.predict(X_test)\n",
    "    \n",
    "    return lr_model, predictions_lr_train, predictions_lr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating a new model\n",
    "multi_nb_improved = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using bernoulli_model function to get the fitted model and prediction scores\n",
    "multinom_model, predictions_nb_improved_train, predictions_nb_improved_test = multi_model(multi_nb_improved,\n",
    "                                                                                              X_train_improved,\n",
    "                                                                                              X_test_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8736616702355461"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scoring the model on training set by comparing it to y_train\n",
    "multinom_model.score(X_train_improved, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111111111111112"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scoring the model on test set by comparing it to y_test\n",
    "multinom_model.score(X_test_improved, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for naive bayes test results with new count vectorizer:\n",
      "\n",
      "[[ 54 169]\n",
      " [ 13 232]]\n",
      "\n",
      "True Negatives: 54\n",
      "False Positives: 169\n",
      "False Negatives: 13\n",
      "True Positives: 232\n",
      "\n",
      "Accuracy: 0.6111111111111112\n",
      "Recall: 0.9469387755102041\n",
      "Precision: 0.5785536159600998\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix for naive bayes test results with new count vectorizer:\")\n",
    "print('')\n",
    "test_confusion_score(predictions_nb_improved_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for naive bayes train results with new count vectorizer:\n",
      "\n",
      "[[643  25]\n",
      " [152 581]]\n",
      "\n",
      "True Negatives: 643\n",
      "False Positives: 25\n",
      "False Negatives: 152\n",
      "True Positives: 581\n",
      "\n",
      "Accuracy: 0.8736616702355461\n",
      "Recall: 0.7926330150068213\n",
      "Precision: 0.9587458745874587\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix for naive bayes train results with new count vectorizer:\")\n",
    "print('')\n",
    "train_confusion_score(predictions_nb_improved_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance has increased dramatically! We were better off without the changes to countvectorizer! Also note the large difference in precision and recall in the test results, with precision being at 57%, meaning many solotravel posts were classified as JapanTravel posts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   14.8s finished\n",
      "C:\\Users\\junho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "#Using lr_model function to get the fitted model and prediction scores\n",
    "lr_model_improved, predictions_lr_improved_train, predictions_lr_improved_test = lr_model(X_train_improved,\n",
    "                                                                                         X_test_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9464668094218416"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score on the improved X_train data\n",
    "lr_model_improved.score(X_train_improved, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6004273504273504"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score on the improved y_test data\n",
    "lr_model_improved.score(X_test_improved, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for logistic regression test results with new count vectorizer:\n",
      "\n",
      "[[158  65]\n",
      " [122 123]]\n",
      "\n",
      "True Negatives: 158\n",
      "False Positives: 65\n",
      "False Negatives: 122\n",
      "True Positives: 123\n",
      "\n",
      "Accuracy: 0.6004273504273504\n",
      "Recall: 0.5020408163265306\n",
      "Precision: 0.6542553191489362\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix for logistic regression test results with new count vectorizer:\")\n",
    "print('')\n",
    "test_confusion_score(predictions_lr_improved_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for logistic regression train results with new count vectorizer:\n",
      "\n",
      "[[660   8]\n",
      " [ 67 666]]\n",
      "\n",
      "True Negatives: 660\n",
      "False Positives: 8\n",
      "False Negatives: 67\n",
      "True Positives: 666\n",
      "\n",
      "Accuracy: 0.9464668094218416\n",
      "Recall: 0.9085948158253752\n",
      "Precision: 0.9881305637982196\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix for logistic regression train results with new count vectorizer:\")\n",
    "print('')\n",
    "train_confusion_score(predictions_lr_improved_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is dramatically overfitting in this case, with a 30% difference in accuracy score between training and test sets! We can also see this difference in the above confusion matrixes, with the training score only having 8 False Positives, which is around 1% of the Negative scores. However, in the test set, there were 65 False Positives, which is about a quarter of the Negative scores!\n",
    "\n",
    "Precision and Recall fell dramatically, with about a 40% difference in Recall and 30% in Precision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try one more countvectorizer, this time just removing the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: CountVectorizer removing stop words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cvec just without stop words\n",
    "cvec = CountVectorizer(max_features = 500, stop_words= 'english')\n",
    "\n",
    "# Apply cvec to the X_train and X_test splits\n",
    "X_train_no_stop = pd.DataFrame(cvec.fit_transform(X_train).toarray(), columns = cvec.get_feature_names())\n",
    "X_test_no_stop = pd.DataFrame(cvec.fit_transform(X_test).toarray(), columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating a new bernoulli model\n",
    "multi_nb_no_stop = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using bernoulli_model function to get the model and prediction scores\n",
    "multi_no_stop_model, predictions_nb_no_stop_train, predictions_nb_no_stop_test = multi_model(multi_nb_no_stop,\n",
    "                                                                                              X_train_no_stop,\n",
    "                                                                                              X_test_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8736616702355461"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the model to score on X_train_no_stop\n",
    "multi_no_stop_model.score(X_train_no_stop, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6025641025641025"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the model to score on X_test_no_stop\n",
    "multi_no_stop_model.score(X_test_no_stop, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it seems there has been an increase in variance compared to the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   14.9s finished\n",
      "C:\\Users\\junho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "#Using lr_model function to get the fitted model and prediction scores\n",
    "lr_model_no_stop, predictions_lr_no_stop_train, predictions_lr_no_stop_test = lr_model(X_train_no_stop,\n",
    "                                                                                         X_test_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9457530335474661"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score on the X_train_no_stop data\n",
    "lr_model_no_stop.score(X_train_no_stop, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.594017094017094"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score on the X_test_no_stop data\n",
    "lr_model_no_stop.score(X_test_no_stop, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the worst version of the logistic regression model so far. There is no need to see the confusion matrix because of just how bad the test score is. It's slightly better than the baseline accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Lemmatizing, using TfidfVectorizer, and using Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far I have been using CountVectorizer to vectorize the words. We have learnt that removing stop words is counter-productive for accuracy.\n",
    "\n",
    "In this section, I move on to using Tf-idf to vectorize the words. I will also use topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1: Lemmatizing the words and using Tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    new to solo travel  post here for introduction...\n",
       "1    hi  can someone tell me how the restrictions w...\n",
       "2    is it expensive traveling with a flexible itin...\n",
       "3    mistakes whilst travelling soloi have personal...\n",
       "4    weak passport holders  how do you travel spont...\n",
       "Name: title_text, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a 'title_text' column that is a combination of 'title' and 'selftext' so it\n",
    "#can be fed to the vectorizer\n",
    "combined_df['title_text'] = combined_df['title'] + combined_df['selftext']\n",
    "combined_df['title_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize 'title_text'\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [[lemmatizer.lemmatize(word) for word in word_tokenize(s)]\n",
    "              for s in combined_df['title_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [new, to, solo, travel, post, here, for, intro...\n",
       "1    [hi, can, someone, tell, me, how, the, restric...\n",
       "2    [is, it, expensive, traveling, with, a, flexib...\n",
       "3    [mistake, whilst, travelling, soloi, have, per...\n",
       "4    [weak, passport, holder, how, do, you, travel,...\n",
       "Name: title_text_lemmatized, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a new column for the lemmatized text\n",
    "combined_df['title_text_lemmatized'] = lemmatized\n",
    "combined_df['title_text_lemmatized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to convert the list of words into a string\n",
    "def converttostr(input_seq, seperator=' '):\n",
    "    # Join all the strings in list\n",
    "    final_str = seperator.join(input_seq)\n",
    "    return (final_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply function to series\n",
    "combined_df['title_text_lemmatized'] = combined_df['title_text_lemmatized'].apply(converttostr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reinstantiate X and y. \n",
    "X = combined_df['title_text_lemmatized']\n",
    "y = combined_df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into X and y train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our Tf-idf vectorizer without stop words\n",
    "tfidfvec = TfidfVectorizer(max_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>...</th>\n",
       "      <th>wondering</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>x200b</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142140</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089331</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00   03   10   11   12   13   14   15   16   17  ...  wondering      work  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.135446   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.000000   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.000000   \n",
       "\n",
       "   world  worth  would  x200b      year  yet       you  your  \n",
       "0    0.0    0.0    0.0    0.0  0.000000  0.0  0.000000   0.0  \n",
       "1    0.0    0.0    0.0    0.0  0.203881  0.0  0.142140   0.0  \n",
       "2    0.0    0.0    0.0    0.0  0.000000  0.0  0.089331   0.0  \n",
       "\n",
       "[3 rows x 500 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform and save the transformed training data into a dataframe\n",
    "X_train_tfidf = pd.DataFrame(tfidfvec.fit_transform(X_train).toarray(), columns = tfidfvec.get_feature_names())\n",
    "X_train_tfidf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>...</th>\n",
       "      <th>wondering</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>x200b</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073058</td>\n",
       "      <td>0.109939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025632</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00   03   10   11   12   13   14        15   16   17  ...  wondering  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  ...        0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  ...        0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.050575  0.0  0.0  ...        0.0   \n",
       "\n",
       "   work  world  worth  would  x200b  year  yet       you      your  \n",
       "0   0.0    0.0    0.0    0.0    0.0   0.0  0.0  0.073058  0.109939  \n",
       "1   0.0    0.0    0.0    0.0    0.0   0.0  0.0  0.000000  0.000000  \n",
       "2   0.0    0.0    0.0    0.0    0.0   0.0  0.0  0.025632  0.000000  \n",
       "\n",
       "[3 rows x 500 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform our testing data with the already-fit tfidfvec\n",
    "X_test_tfidf = pd.DataFrame(tfidfvec.transform(X_test).toarray(), columns = tfidfvec.get_feature_names())\n",
    "X_test_tfidf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2: Using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating a Multinomial Naive Bayesmodel\n",
    "multi_nb_tfidf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Multinomial Naive Bayes model\n",
    "multi_nb_tfidf, predictions_nb_tfidf_train, predictions_nb_tfidf_test = multi_model(multi_nb_tfidf,\n",
    "                                                                                              X_train_tfidf,\n",
    "                                                                                              X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9172019985724482"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes training score\n",
    "multi_nb_tfidf.score(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9123931623931624"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes test score\n",
    "multi_nb_tfidf.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for naive bayes test results with tfidf and word lemmatization:\n",
      "\n",
      "[[211  12]\n",
      " [ 29 216]]\n",
      "\n",
      "True Negatives: 211\n",
      "False Positives: 12\n",
      "False Negatives: 29\n",
      "True Positives: 216\n",
      "\n",
      "Accuracy: 0.9123931623931624\n",
      "Recall: 0.8816326530612245\n",
      "Precision: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix for naive bayes test results with tfidf and word lemmatization:\")\n",
    "print('')\n",
    "test_confusion_score(predictions_nb_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for naive bayes train results with tfidf and word lemmatization:\n",
      "\n",
      "[[648  20]\n",
      " [ 96 637]]\n",
      "\n",
      "True Negatives: 648\n",
      "False Positives: 20\n",
      "False Negatives: 96\n",
      "True Positives: 637\n",
      "\n",
      "Accuracy: 0.9172019985724482\n",
      "Recall: 0.869031377899045\n",
      "Precision: 0.969558599695586\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix for naive bayes train results with tfidf and word lemmatization:\")\n",
    "print('')\n",
    "train_confusion_score(predictions_nb_tfidf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both bias and variance have massively dropped! We have reached 91% accuracy. Note that False Negatives are still higher than False Positives, so JapanTravel posts are still more commonly mis-classified as solotravel posts than vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2: Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    2.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Run the Logistic Regression model on the training and test sets\n",
    "lr_model_tfidf, predictions_lr_tfidf_train, predictions_lr_tfidf_test = lr_model(X_train_tfidf,\n",
    "                                                                                         X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9607423269093505"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score on the X_train_tfidf training data\n",
    "lr_model_tfidf.score(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9487179487179487"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score on the X_train_tfidf test data\n",
    "lr_model_tfidf.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for Logistic Regression test results with tfidf and word lemmatization:\n",
      "\n",
      "[[211  12]\n",
      " [ 12 233]]\n",
      "\n",
      "True Negatives: 211\n",
      "False Positives: 12\n",
      "False Negatives: 12\n",
      "True Positives: 233\n",
      "\n",
      "Accuracy: 0.9487179487179487\n",
      "Recall: 0.9510204081632653\n",
      "Precision: 0.9510204081632653\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix for Logistic Regression test results with tfidf and word lemmatization:\")\n",
    "print('')\n",
    "test_confusion_score(predictions_lr_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for Logistic Regression train results with tfidf and word lemmatization:\n",
      "\n",
      "[[647  21]\n",
      " [ 34 699]]\n",
      "\n",
      "True Negatives: 647\n",
      "False Positives: 21\n",
      "False Negatives: 34\n",
      "True Positives: 699\n",
      "\n",
      "Accuracy: 0.9607423269093505\n",
      "Recall: 0.9536152796725784\n",
      "Precision: 0.9708333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix for Logistic Regression train results with tfidf and word lemmatization:\")\n",
    "print('')\n",
    "train_confusion_score(predictions_lr_tfidf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw an ROC AUC graph for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the False Positive Rate (fpr), True Positive Rates (tpr), and the threshold\n",
    "fpr, tpr, threshold = roc_curve(y_test, predictions_lr_tfidf_test)\n",
    "\n",
    "# Setting the auc curve\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc curve for this Logistic Regression, ElasticNet model\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9486043744852201\n"
     ]
    }
   ],
   "source": [
    "# Getting the ROC AUC curvve\n",
    "print(\"Roc curve for this Logistic Regression, ElasticNet model\\n\")\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "\n",
    "# Put a legend at the bottom right\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "\n",
    "# Set up graph window\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Set up x and y labels\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are even better than the Naive Bayes model. The same ideas we've found apply here too, with False Negatives being higher than False Positives. However, this Logistic Regression model has reduced the disparity in the two!\n",
    "\n",
    "The AUC curve represents how well the model did. The larger the area enclosed below the blue line, the better. This means that when the blue line is a right-angled triangle with the right-angle at the top left hand corner, we get a perfect score. This curve has a score of 94.86%, which is the accuracy of the Logistic Regression model predicting on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this Logistic Regression model with the Pickle library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into the production_model folder\n",
    "filename = '../production_model/production_model_lr'\n",
    "outfile = open(filename,'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the model\n",
    "pickle.dump(lr_model_tfidf,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.1: Analysing the production model and how we obtained the score we did"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did removing stop words make things worse\n",
    "\n",
    "We saw that removing stop words reduced performance of both classifiers significantly; both bias and variance increased, particularly the variance. As we saw in the data exploration, Question 2 (\"How do the shortest strings look like?\"), there are a significant number of stop words used in JapanTravel. Its top most frequent word, 'the', is mentioned over 15,000 times, compared to solotravel's use of 'the', which is 3rd most frequent at only 5,000 or so usages. Since these stop words are mentioned around 2-3 times more often in JapanTravel, the presence of stop words gives the models useful information in the form of number of times they are used in a post.\n",
    "\n",
    "This is further augmented by our analysis in Question 3 (\"How do the shortest strings look like?\"), Question 5 (\"How many unique and non-unique tokens are there?\") and Question 6 (\"What is the average length of tokens?\"), which finds that both subreddits have an average token length of around 3-4, with JapanTravel having 3 times as many tokens. Removing stop words removed a large number of these tokens, giving the models less information to work with.\n",
    "\n",
    "### How did we manage to get the production model?\n",
    "\n",
    "#### Adding 'title' to 'selftext' gave the model more information to work with\n",
    "\n",
    "I think the biggest factor was adding the post titles to the main text, giving both subreddits more information to work with. This is significant because some posts in selftext were empty, which I had replaced with \"no_text\". There were more posts in solotravel that had no_text compared to JapanTravel, which could explain why there were more False Negatives before title information was added in. Since a title was mandatory, and, as we explored in question 7 (\"What are the most common unigrams, bigrams, and trigrams?\"), different topics were discussed, the words in the titles would be different from one another.\n",
    "\n",
    "#### Lemmatizing words increased token frequency\n",
    "\n",
    "The second factor was lemmatizing words. This reduced different forms of words to its lemma, the 'head word' used in a dictionary. This means there were fewer different versions of words, which increased token frequency. This difference in number is reflected in the different number of words in both subreddits.\n",
    "\n",
    "#### Tf-idf vectorizer better than Count vectorizer\n",
    "\n",
    "I initially used CountVectorizer to vectorize the posts. However, I used Tf-idf vectorizer in the final production model. CountVectorizer simply counts the frequency of a word, which means that rare words will be penalised. Tf-idf vectorzier considers the overall weight of words in comparison to each document (post in our case). It weighs words in comparison to how often they appear in documents, so rare words that appear in several documents a few times will still get a good weight.\n",
    "\n",
    "I also used a max_feature argument of 500, so only the top 500 words were considered. This prevents overfitting.\n",
    "\n",
    "#### Using ElasticNet prevented overfitting\n",
    "\n",
    "Remember that we didn't use vanilla Logistic Regression, we used the ElasticNet version of it. This means that it uses a combination of Ridge and Lasso regression, giving weights to the 500 features and sometimes zero-ing them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.2: Analysing what we can do with this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can this project help companies?\n",
    "\n",
    "With a production model in place, companies can now try to scrape new data from the subreddits and apply the production model to the text data. Given an approximately 95% accuracy/recall/precision score, companies will still have to look out for the 5% posts that will not be correctly categorised. More attention will have to be directed towards the solotravel subreddit, where JapanTravel posts are more likely to be categorised in.\n",
    "\n",
    "The analysis we did in the previous notebook gives companies a sense of topics that both JapanTravel and solotravel are interested in.\n",
    "\n",
    "### What else can companies do now?\n",
    "\n",
    "1. Apply this production model to a new set of 25 posts from Reddit's API. They should expect about 4 misclassified posts in a set of 50 from 2 subreddits.\n",
    "\n",
    "2. Set up a notification system with python which will alert employees to a new post.\n",
    "\n",
    "3. Use text analysis methodology from the Social Sciences to analyse text from the posts. NLP methods will be needed here, such as using sentiment analysis to find out how people feel about a particular topic.\n",
    "\n",
    "4. Set up a new production model for other subreddits with these notebooks as a template. Simply replace the variables with the appropriate heading and csv file names.\n",
    "\n",
    "5. If Logistic Regression with ElasticNet isn't enough, work on another set of classifiers. Some suggestions are a Random Forest classifier and K-Nearest Neighbor.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
